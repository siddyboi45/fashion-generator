# -*- coding: utf-8 -*-
"""CDCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_wf8x0Dyfxk51Oy1SwYTopPU-r2NnFGH

## Conditional DCGAN Implementation on DeepFashion Dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# import require libraries

import shutil
import json
import os
from google.colab import drive
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image
import pickle
drive.mount('/content/drive')

import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from IPython.display import HTML
# %matplotlib inline

import torch
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms

from torchvision import datasets, transforms

# Instead of using the entire dataset. Due to computing constraints, we only use the train images folder with 14000 images
categories = [x.split(' ')[0] for x in open('/content/cropped/Anno_coarse/list_category_cloth.txt').readlines()[2:]]
category_ids = open('/content/cropped/Anno_fine/train_cate.txt').readlines()
img_cat_ids = open('/content/cropped/Anno_fine/train.txt').readlines()

# Move the cropped images the train_imgs folder to which will be used for training the model
save_root = '/content/cropped/'

for idx in range(len(img_cat_ids)):
  image_name = img_cat_ids[idx][:-4]+'png'
  category_id = eval(category_ids[idx])
  if not (os.path.exists(save_root + 'train_imgs/' + categories[category_id-1])):
    os.makedirs(save_root + 'train_imgs/' + categories[category_id-1])
  if not (os.path.exists(save_root + image_name)):
    continue
  if not (os.path.exists(save_root + 'train_imgs/' + categories[category_id-1] + '/' + image_name.split('/')[2])):
    s = image_name.split('/')
    os.rename(save_root + image_name, save_root + s[0] + '/' + s[1] + s[2])
    shutil.copy(os.path.join(save_root + image_name, save_root + s[0] + '/' + s[1] + s[2]), os.path.join(save_root + 'train_imgs/' + categories[category_id-1]))

# Count number of images in this folder

x = os.listdir('/content/cropped/train_imgs/')
files = 0

for i in x:
  if i =='img.zip':
    continue
  files = files + len(os.listdir('/content/cropped/train_imgs/'+i))

print(files)

# Check and use GPU if available
DEVICE = torch.device("cuda" if torch.cuda.is_available()
                      else "cpu")
print(f"Using {DEVICE} backend")

# Set batch size for training models. We use the same batch size as DCGAN implementation
BATCH_SIZE = 128

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Perform basic preprocessing as done in DCGAN implementation
# # Data is resized to 64x64, centercropped and normalized
# transform = transforms.Compose([transforms.Resize(64),
#                                 transforms.CenterCrop(64),
#                                 transforms.ToTensor(),
#                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
#                                ])
# 
# # Data root is the train_imgs folder
# dataroot = "/content/cropped/train_imgs/"
# train_data = datasets.ImageFolder(root = dataroot, transform = transform)

# Create ImageFolder with the dataroor and transform as above
dataroot = "/content/cropped/train_imgs/"
train_data = datasets.ImageFolder(root = dataroot, transform = transform)
# Create the dataloader
dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,
                                         shuffle=True, num_workers=2, drop_last=True)

# Generate and store sample data as the input_grid
dataiter = iter(dataloader)
images, labels = next(dataiter)

img_size = images.shape[2]
print(images.shape)
torchvision.utils.save_image(images, f"input_grid.jpg", nrow=16, padding=0, normalize=True)

# Plot the input images from above
plt.figure(figsize=(16, BATCH_SIZE/16))
plt.axis("off")
plt.title("Training Images")
_ = plt.imshow(Image.open(f"input_grid.jpg"))

# Discriminator code for Conditional DCGAN
# Architecture is based on DCGAN Paper implementation
# Another layer added for inputting category labels
# Modified for input and output sizes accordingly
# Ref: https://colab.research.google.com/github/drc10723/GAN_design/blob/master/GAN_implementations/Conditional_DCGAN_MNIST.ipynb


class Discriminator(nn.Module):
  """ D(x) """
  def __init__(self):
    # initalize super module
    super(Discriminator, self).__init__()
    
    # Image layer to take input images
    self.layer_x = nn.Sequential(nn.Conv2d(in_channels=3, out_channels=32,
                                           kernel_size=4, stride=2, padding=1, bias=False),

                                 nn.LeakyReLU(0.2, inplace=True),

                                )
    
    # Label layer to take conditional labels for each image
    self.layer_y = nn.Sequential(nn.Conv2d(in_channels=50, out_channels=32,
                                           kernel_size=4, stride=2, padding=1, bias=False),

                                 nn.LeakyReLU(0.2, inplace=True),

                                 )
    
    # Concat the above image layer and label layer
    self.layer_xy = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=64*2,
                                            kernel_size=4, stride=2, padding=1, bias=False),

                               nn.BatchNorm2d(64*2),

                               nn.LeakyReLU(0.2, inplace=True),

                               nn.Conv2d(in_channels=64*2, out_channels=64*4,
                                         kernel_size=4, stride=2, padding=1, bias=False),

                               nn.BatchNorm2d(64*4),

                               nn.LeakyReLU(0.2, inplace=True),

                               nn.Conv2d(in_channels=64*4, out_channels=1,
                                         kernel_size=8, stride=1, padding=0, bias=False),
                               
                               nn.Sigmoid()
                               )
  
  def forward(self, x, y):

    x = self.layer_x(x)
    # print(x.shape)
    
    y = self.layer_y(y)
    # print(y.shape)
    
    # Concat the above image layer and label layer
    xy = torch.cat([x,y], dim=1)
    # print('xy shape after cat', xy.shape)
    # size of xy : (batch_size, 64, 14, 14)
    xy = self.layer_xy(xy)
    # size of xy : (batch_size, 1, 1, 1)
    # print('xy shape before view', xy.shape)
    # print('xy shape[0] before view', xy.shape[0])
    xy = xy.view(xy.shape[0], -1)
    # size of xy : (batch_size, 1)
    # print('xy shape[0] after view', xy.shape)
    return xy


# Initialize the Discriminator object
netD = Discriminator().to(DEVICE)

# Model summary
print(netD)

# Generator code for Conditional DCGAN
# Architecture is based on DCGAN Paper implementation
# Another layer added for inputting category labels
# Modified for input and output sizes accordingly
# Ref: https://colab.research.google.com/github/drc10723/GAN_design/blob/master/GAN_implementations/Conditional_DCGAN_MNIST.ipynb

class Generator(nn.Module):
  """ G(z) """
  def __init__(self, input_size=100):
    # initalize super module
    super(Generator, self).__init__()

    # Image layer to take input images
    self.layer_x = nn.Sequential(nn.ConvTranspose2d(in_channels=500, out_channels=128, kernel_size=8,
                                                  stride=2, padding=0, bias=False),

                                 nn.BatchNorm2d(64*2),

                                 nn.ReLU(),

                                )
    
    # Label layer to take conditional labels for each image
    self.layer_y = nn.Sequential(nn.ConvTranspose2d(in_channels=50, out_channels=128, kernel_size=8,
                                                  stride=2, padding=0, bias=False),

                                 nn.BatchNorm2d(64*2),

                                 nn.ReLU(),

                                )
    
    # Concat the above image layer and label layer
    self.layer_xy = nn.Sequential(nn.ConvTranspose2d(in_channels=64*4, out_channels=64*2, kernel_size=3,
                                                  stride=2, padding=0, bias=False),

                               nn.BatchNorm2d(64*2),

                               nn.ReLU(),

                               nn.ConvTranspose2d(in_channels=64*2, out_channels=64, kernel_size=3,
                                                  stride=2, padding=1, bias=False),

                               nn.BatchNorm2d(64),

                               nn.ReLU(),

                               nn.ConvTranspose2d(in_channels=64, out_channels=3, kernel_size=2,
                                                  stride=2, padding=1, bias=False),

                               nn.Tanh())

    
  def forward(self, x, y):

    x = x.view(x.shape[0], x.shape[1], 1, 1)

    x = self.layer_x(x)

    y = y.view(y.shape[0], y.shape[1], 1, 1)

    y = self.layer_y(y)
    
    # Concat the above image layer and label layer
    xy = torch.cat([x,y], dim=1)
    xy = self.layer_xy(xy)
    return xy


# Initialize the Generator object
netG = Generator().to(DEVICE)

# Model Summary
print(netG)

# custom weights initialization
def weights_init(net):
    classname = net.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(net.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(net.weight.data, 1.0, 0.02)
        nn.init.constant_(net.bias.data, 0)

# Randomly assign weights using weight_init
netD.apply(weights_init)
netG.apply(weights_init)

# Number of epochs for training
NUM_EPOCH = 200
# size of latent vector z
size_z = 500
# number of discriminator steps for each generator step
Ksteps = 1
# Hyperparameters of optimizer as per DCGAN recommendation
Adam_lr = 0.0002

Adam_beta1 = 0.5

# We have chosen the Binray Cross Entropy Loss since it is the most commonly used loss function
criterion = nn.BCELoss()

optimizerG = torch.optim.Adam(netG.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))

optimizerD = torch.optim.Adam(netD.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))

# Set real and fake labels
labels_real = torch.ones((BATCH_SIZE, 1)).to(DEVICE)

labels_fake = torch.zeros((BATCH_SIZE, 1)).to(DEVICE)

# Generate random noise which will be used as generator input
z_test = torch.randn(100, size_z).to(DEVICE)

# To structure labels we followed same approach as MNIST conditional DCGAN
# Input sizes are scaled from 10 to 50 accordingly
# Convert labels to onehot encoding
onehot = torch.zeros(50, 50).scatter_(1, torch.tensor(list(range(50))).view(50,1), 1)
# Reshape labels to image size, with number of labels as channel
fill = torch.zeros([50, 50, img_size, img_size])
# Channel corresponding to label will be set one and all other zeros
for i in range(50):
    fill[i, i, :, :] = 1
# Create labels for testing generator
test_y = torch.tensor(list(range(50))*2).type(torch.LongTensor)
# Convert to one hot encoding
test_Gy = onehot[test_y].to(DEVICE)

# Initialize losses to empty lists. They get appended in each epoch
D_losses = []
G_losses = []
Dx_values = []
DGz_values = []

# number of training steps done on discriminator 
step = 0
for epoch in range(NUM_EPOCH):
  epoch_D_losses = []
  epoch_G_losses = []
  epoch_Dx = []
  epoch_DGz = []
  # iterate through data loader generator object
  for images, y_labels in dataloader:
    step += 1
    ############################
    # Update D network: maximize log(D(x)) + log(1 - D(G(z)))
    ###########################
    # images will be send to gpu, if cuda available
    x = images.to(DEVICE)
    # preprocess labels for feeding as y input
    D_y = fill[y_labels].to(DEVICE)
    # forward pass D(x)
    x_preds = netD(x, D_y)
    # calculate loss log(D(x))
    D_x_loss = criterion(x_preds, labels_real)
    
    # create latent vector z from normal distribution 
    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)
    # create random y labels for generator
    y_gen = (torch.rand(BATCH_SIZE, 1)*10).type(torch.LongTensor).squeeze()
    # convert genarator labels to onehot
    G_y = onehot[y_gen].to(DEVICE)
    # preprocess labels for feeding as y input in D
    DG_y = fill[y_gen].to(DEVICE)
    
    # generate image
    fake_image = netG(z, G_y)
    # calculate D(G(z)), fake or not
    z_preds = netD(fake_image.detach(), DG_y)
    # loss log(1 - D(G(z)))
    D_z_loss = criterion(z_preds, labels_fake)
    
    # total loss = log(D(x)) + log(1 - D(G(z)))
    D_loss = D_x_loss + D_z_loss
    
    # save values for plots
    epoch_D_losses.append(D_loss.item())
    epoch_Dx.append(x_preds.mean().item())
    
    # zero accumalted grads
    netD.zero_grad()
    # do backward pass
    D_loss.backward()
    # update discriminator model
    optimizerD.step()
    
    ############################
    # Update G network: maximize log(D(G(z)))
    ###########################
        
    # if Ksteps of Discriminator training are done, update generator
    if step % Ksteps == 0:
      # As we done one step of discriminator, again calculate D(G(z))
      z_out = netD(fake_image, DG_y)
      # loss log(D(G(z)))
      G_loss = criterion(z_out, labels_real)
      # save values for plots
      epoch_DGz.append(z_out.mean().item())
      epoch_G_losses.append(G_loss)
      
      # zero accumalted grads
      netG.zero_grad()
      # do backward pass
      G_loss.backward()
      # update generator model
      optimizerG.step()
  else:
    # calculate average value for one epoch
    D_losses.append(sum(epoch_D_losses)/len(epoch_D_losses))
    G_losses.append(sum(epoch_G_losses)/len(epoch_G_losses))
    Dx_values.append(sum(epoch_Dx)/len(epoch_Dx))
    DGz_values.append(sum(epoch_DGz)/len(epoch_DGz))
    
    print(f" Epoch {epoch+1}/{NUM_EPOCH} Discriminator Loss {D_losses[-1]:.3f} Generator Loss {G_losses[-1]:.3f}"
         + f" D(x) {Dx_values[-1]:.3f} D(G(x)) {DGz_values[-1]:.3f}")
    
    # Generating images after each epoch and saving
    # set generator to evaluation mode
    netG.eval()
    with torch.no_grad():
      # forward pass of G and generated image
      fake_test = netG(z_test, test_Gy).cpu()
      # Save images
      torchvision.utils.save_image(fake_test, f"/content/drive/MyDrive/C-DCGAN_epoch_{epoch+1}.jpg", nrow=10, padding=0, normalize=True)
    # set generator to training mode
    netG.train()
  
  # For each 50th iteration save losses and current generator output
  if step%50 == 0:
    with open('/content/drive/MyDrive/epoch_G_losses.pkl', 'wb') as f:
      pickle.dump(epoch_G_losses, f)
    f.close()
    with open('/content/drive/MyDrive/epoch_DGz.pkl', 'wb') as f:
      pickle.dump(epoch_DGz, f)
    f.close()
    fake_test = netG(z_test, test_Gy).cpu()
    torchvision.utils.save_image(fake_test, f"/content/drive/MyDrive/C-DCGAN_step_{step+1}_epoch_{epoch+1}.jpg", nrow=10, padding=0, normalize=True)

# Plot Generator output with random noise as input
print(test_Gy.shape)
print(z_test.shape)

fake_test = netG(z_test, test_Gy).cpu()
torchvision.utils.save_image(fake_test, f"/content/C-DCGAN_step_{step+1}_epoch_{epoch+1}.jpg", nrow=10, padding=0, normalize=True)
plt.imshow(Image.open(f"/content/C-DCGAN_step_{step+1}_epoch_{epoch+1}.jpg"))